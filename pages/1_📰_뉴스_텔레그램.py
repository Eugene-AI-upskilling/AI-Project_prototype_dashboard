# coding=utf-8
"""
í˜ì´ì§€ 1: ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨ - ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì§€ì›
"""

import streamlit as st
import pandas as pd
import os
import re
import time
from datetime import datetime
from typing import List, Dict, Optional
from io import BytesIO

import requests
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

st.set_page_config(page_title="ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨", page_icon="ğŸ“°", layout="wide")


# =============================================================================
# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
# =============================================================================

NEWS_SOURCES = {
    "ë„¤ì´ë²„ ë‰´ìŠ¤": {
        "type": "naver_api",
        "description": "ë„¤ì´ë²„ ë‰´ìŠ¤ API (í‚¤ì›Œë“œ ê²€ìƒ‰)",
        "language": "ko"
    },
    # êµ­ë‚´ ë°”ì´ì˜¤ ì „ë¬¸ì§€
    "ë”ë°”ì´ì˜¤": {
        "type": "web_scrape",
        "url": "https://www.thebionews.net/",
        "description": "êµ­ë‚´ ë°”ì´ì˜¤ ì „ë¬¸ ë‰´ìŠ¤",
        "language": "ko",
        "category": "ë°”ì´ì˜¤"
    },
    "íˆíŠ¸ë‰´ìŠ¤": {
        "type": "web_scrape",
        "url": "https://www.hitnews.co.kr/",
        "description": "êµ­ë‚´ í—¬ìŠ¤ì¼€ì–´/ì œì•½ ë‰´ìŠ¤",
        "language": "ko",
        "category": "ë°”ì´ì˜¤"
    },
    "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸": {
        "type": "web_scrape",
        "url": "https://www.hankyung.com/bioinsight",
        "description": "í•œêµ­ê²½ì œ ë°”ì´ì˜¤ ì„¹ì…˜",
        "language": "ko",
        "category": "ë°”ì´ì˜¤"
    },
    # ì™¸ì‹  ë°”ì´ì˜¤/í—¬ìŠ¤ì¼€ì–´
    "Reuters Healthcare": {
        "type": "web_scrape",
        "url": "https://www.reuters.com/business/healthcare-pharmaceuticals/",
        "description": "ë¡œì´í„° í—¬ìŠ¤ì¼€ì–´/ì œì•½",
        "language": "en",
        "category": "ë°”ì´ì˜¤"
    },
    "FierceBiotech": {
        "type": "web_scrape",
        "url": "https://www.fiercebiotech.com/",
        "description": "ë°”ì´ì˜¤í… ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ë°”ì´ì˜¤"
    },
    "FiercePharma": {
        "type": "web_scrape",
        "url": "https://www.fiercepharma.com/",
        "description": "ì œì•½ ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ë°”ì´ì˜¤"
    },
    # ì™¸ì‹  IT/í…Œí¬
    "TrendForce": {
        "type": "web_scrape",
        "url": "https://www.trendforce.com/",
        "description": "ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´ ì‹œì¥ ë¶„ì„",
        "language": "en",
        "category": "IT"
    },
    "DigiTimes": {
        "type": "web_scrape",
        "url": "https://www.digitimes.com/",
        "description": "ì•„ì‹œì•„ í…Œí¬ ì‚°ì—… ë‰´ìŠ¤",
        "language": "en",
        "category": "IT"
    },
    "The Register": {
        "type": "web_scrape",
        "url": "https://www.theregister.com/",
        "description": "IT/ì—”í„°í”„ë¼ì´ì¦ˆ ë‰´ìŠ¤",
        "language": "en",
        "category": "IT"
    },
    "Constellation Research": {
        "type": "web_scrape",
        "url": "https://www.constellationr.com/",
        "description": "ê¸°ì—… IT ë¦¬ì„œì¹˜",
        "language": "en",
        "category": "IT"
    },
    "Phys.org": {
        "type": "web_scrape",
        "url": "https://phys.org/",
        "description": "ê³¼í•™/ê¸°ìˆ  ë‰´ìŠ¤",
        "language": "en",
        "category": "IT"
    },
}

# ì–¸ë¡ ì‚¬ ê·¸ë£¹
SOURCE_GROUPS = {
    "ë„¤ì´ë²„ ë‰´ìŠ¤ (í‚¤ì›Œë“œ ê²€ìƒ‰)": ["ë„¤ì´ë²„ ë‰´ìŠ¤"],
    "êµ­ë‚´ ë°”ì´ì˜¤ ì „ë¬¸ì§€": ["ë”ë°”ì´ì˜¤", "íˆíŠ¸ë‰´ìŠ¤", "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸"],
    "ì™¸ì‹  ë°”ì´ì˜¤/í—¬ìŠ¤ì¼€ì–´": ["Reuters Healthcare", "FierceBiotech", "FiercePharma"],
    "ì™¸ì‹  IT/í…Œí¬": ["TrendForce", "DigiTimes", "The Register", "Constellation Research", "Phys.org"],
}


# =============================================================================
# í™˜ê²½ë³€ìˆ˜/Secrets
# =============================================================================

def get_secret(key, default=None):
    """Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
    try:
        value = st.secrets.get(key)
        if value:
            return value
    except Exception:
        pass

    value = os.environ.get(key)
    if value:
        return value

    return default


# =============================================================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
# =============================================================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


def search_naver_news(keyword: str, display: int = 10) -> List[Dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ê²€ìƒ‰"""
    client_id = get_secret('NAVER_CLIENT_ID')
    client_secret = get_secret('NAVER_CLIENT_SECRET')

    if not client_id or not client_secret:
        st.error("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {
        "query": keyword,
        "display": display,
        "sort": "date"
    }

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = []
        for item in data.get('items', []):
            title = item['title'].replace('<b>', '').replace('</b>', '')
            description = item['description'].replace('<b>', '').replace('</b>', '')

            results.append({
                'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
                'keyword': keyword,
                'title': title,
                'description': description,
                'link': item['originallink'] or item['link'],
                'pubDate': item['pubDate'],
                'language': 'ko'
            })

        return results

    except Exception as e:
        st.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def scrape_news_site(source_name: str, max_items: int = 10) -> List[Dict]:
    """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
    source = NEWS_SOURCES.get(source_name)
    if not source:
        return []

    url = source.get('url')
    language = source.get('language', 'ko')

    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        results = []

        # ì‚¬ì´íŠ¸ë³„ íŒŒì‹± ë¡œì§
        if source_name == "ë”ë°”ì´ì˜¤":
            articles = soup.select('div.article-list article, div.news-list li, .post-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', '.title', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = url.rstrip('/') + '/' + link.lstrip('/')
                    if title:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "íˆíŠ¸ë‰´ìŠ¤":
            articles = soup.select('ul.type01 li, div.list-block article, .news-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', 'a', '.tit'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.hitnews.co.kr' + link
                    if title:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸":
            articles = soup.select('div.news-list li, article.news-item, .article-list li')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', 'a', '.tit'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.hankyung.com' + link
                    if title:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "Reuters Healthcare":
            articles = soup.select('article, div[data-testid="MediaStoryCard"], .story-card')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h3', 'h2', 'span[data-testid="Heading"]', '.title'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.reuters.com' + link
                    if title and len(title) > 10:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name in ["FierceBiotech", "FiercePharma"]:
            articles = soup.select('article.node, div.view-content article, .article-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', '.field-title', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = url.rstrip('/') + link
                    if title and len(title) > 10:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "TrendForce":
            articles = soup.select('article, .news-item, div.post')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', '.title', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.trendforce.com' + link
                    if title and len(title) > 5:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "DigiTimes":
            articles = soup.select('div.col_body article, .article-list li, .news-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', 'a', '.title'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.digitimes.com' + link
                    if title and len(title) > 5:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "The Register":
            articles = soup.select('article, .story_list article, div.story')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', '.headline', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.theregister.com' + link
                    if title and len(title) > 10:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "Constellation Research":
            articles = soup.select('article, .post, .blog-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', '.title', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://www.constellationr.com' + link
                    if title and len(title) > 5:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        elif source_name == "Phys.org":
            articles = soup.select('article.news-item, div.sorted-article, .article-item')[:max_items]
            for article in articles:
                link_elem = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', '.title', 'a'])
                if link_elem and title_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://phys.org' + link
                    if title and len(title) > 10:
                        results.append({
                            'source': source_name,
                            'title': title,
                            'link': link,
                            'language': language
                        })

        # ì¼ë°˜ì ì¸ íŒŒì‹± (ìœ„ì—ì„œ ëª» ì°¾ì€ ê²½ìš°)
        if not results:
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì•„í‹°í´ íŒ¨í„´
            for tag in ['article', 'div.news', 'div.post', 'li.article']:
                articles = soup.select(tag)[:max_items]
                for article in articles:
                    link_elem = article.find('a')
                    title_elem = article.find(['h1', 'h2', 'h3', 'h4'])
                    if link_elem and title_elem:
                        title = title_elem.get_text(strip=True)
                        link = link_elem.get('href', '')
                        if title and len(title) > 5:
                            if not link.startswith('http'):
                                link = url.rstrip('/') + '/' + link.lstrip('/')
                            results.append({
                                'source': source_name,
                                'title': title,
                                'link': link,
                                'language': language
                            })
                if results:
                    break

        return results[:max_items]

    except Exception as e:
        st.warning(f"{source_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        return []


def translate_and_summarize(news_items: List[Dict], source_name: str) -> str:
    """ì™¸ì‹  ë‰´ìŠ¤ ë²ˆì—­ ë° ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')

    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:7]])

        prompt = f"""ë‹¤ìŒì€ '{source_name}'ì˜ ìµœì‹  í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

[ìš”êµ¬ì‚¬í•­]
- ê° í—¤ë“œë¼ì¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­
- ì „ì²´ì ì¸ íŠ¸ë Œë“œ/í•µì‹¬ ì´ìŠˆë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- íˆ¬ìì/ì—…ê³„ ê´€ê³„ì ê´€ì ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ ê°•ì¡°

[í—¤ë“œë¼ì¸]
{news_text}

[ì¶œë ¥ í˜•ì‹]
## ì£¼ìš” í—¤ë“œë¼ì¸ (ë²ˆì—­)
- (ë²ˆì—­1)
- (ë²ˆì—­2)
...

## í•µì‹¬ ìš”ì•½
(2-3ë¬¸ì¥ ìš”ì•½)
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ/ë°”ì´ì˜¤/IT ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        return response.choices[0].message.content

    except Exception as e:
        return f"ë²ˆì—­/ìš”ì•½ ì‹¤íŒ¨: {str(e)}"


def summarize_korean_news(news_items: List[Dict], keyword: str) -> str:
    """êµ­ë‚´ ë‰´ìŠ¤ ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')

    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:5]])

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³  í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": f"í‚¤ì›Œë“œ: {keyword}\n\në‰´ìŠ¤:\n{news_text}"}
            ],
            temperature=0.3,
            max_tokens=200
        )

        return response.choices[0].message.content

    except Exception as e:
        return None


def send_telegram(message: str) -> bool:
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    bot_token = get_secret('BOT_TOKEN')
    chat_id = get_secret('CHAT_ID')

    if not bot_token or not chat_id:
        return False

    url = f"https://api.telegram.org/bot{bot_token.strip()}/sendMessage"

    try:
        response = requests.post(url, data={
            'chat_id': chat_id.strip(),
            'text': message[:4096],
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }, timeout=30)

        return response.status_code == 200

    except:
        return False


# =============================================================================
# ë©”ì¸ ì•±
# =============================================================================

def main():
    st.title("ğŸ“° ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨")
    st.markdown("ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ GPT ìš”ì•½/ë²ˆì—­ â†’ í…”ë ˆê·¸ë¨ ë°œì†¡")

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2 = st.tabs(["ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘", "âš™ï¸ ì†ŒìŠ¤ ê´€ë¦¬"])

    with tab1:
        # ì†ŒìŠ¤ ì„ íƒ
        st.subheader("ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ")

        col1, col2 = st.columns(2)

        with col1:
            selected_groups = st.multiselect(
                "ì†ŒìŠ¤ ê·¸ë£¹ ì„ íƒ",
                list(SOURCE_GROUPS.keys()),
                default=["ë„¤ì´ë²„ ë‰´ìŠ¤ (í‚¤ì›Œë“œ ê²€ìƒ‰)"]
            )

        with col2:
            # ê°œë³„ ì†ŒìŠ¤ ì„ íƒ
            all_sources_in_groups = []
            for group in selected_groups:
                all_sources_in_groups.extend(SOURCE_GROUPS[group])

            if all_sources_in_groups:
                selected_sources = st.multiselect(
                    "ê°œë³„ ì†ŒìŠ¤ ì„ íƒ (ì„ íƒí•œ ê·¸ë£¹ ë‚´)",
                    all_sources_in_groups,
                    default=all_sources_in_groups
                )
            else:
                selected_sources = []

        st.markdown("---")

        # ë„¤ì´ë²„ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì„¤ì • (ë„¤ì´ë²„ ì„ íƒ ì‹œ)
        if "ë„¤ì´ë²„ ë‰´ìŠ¤" in selected_sources:
            st.subheader("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í‚¤ì›Œë“œ")
            keywords = st.text_area(
                "ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                value="ì‚¼ì„±ì „ì\nSKí•˜ì´ë‹‰ìŠ¤\në°˜ë„ì²´",
                height=100
            )
        else:
            keywords = ""

        # ì˜µì…˜
        st.subheader("âš™ï¸ ì˜µì…˜")
        col1, col2, col3 = st.columns(3)

        with col1:
            max_news = st.slider("ì†ŒìŠ¤ë‹¹ ë‰´ìŠ¤ ìˆ˜", 3, 20, 10)

        with col2:
            use_gpt = st.checkbox("GPT ìš”ì•½/ë²ˆì—­", value=True)

        with col3:
            send_to_telegram = st.checkbox("í…”ë ˆê·¸ë¨ ë°œì†¡", value=False)

        st.markdown("---")

        # ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True):
            if not selected_sources:
                st.error("ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return

            all_news = []
            summaries = {}

            progress_bar = st.progress(0)
            status_text = st.empty()

            total_steps = len(selected_sources)
            if "ë„¤ì´ë²„ ë‰´ìŠ¤" in selected_sources and keywords:
                keyword_list = [k.strip() for k in keywords.split('\n') if k.strip()]
                total_steps += len(keyword_list) - 1  # ë„¤ì´ë²„ëŠ” í‚¤ì›Œë“œë³„ë¡œ ìˆ˜ì§‘

            current_step = 0

            for source_name in selected_sources:
                if source_name == "ë„¤ì´ë²„ ë‰´ìŠ¤":
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ëŠ” í‚¤ì›Œë“œë³„ ìˆ˜ì§‘
                    keyword_list = [k.strip() for k in keywords.split('\n') if k.strip()]
                    for keyword in keyword_list:
                        current_step += 1
                        status_text.text(f"ìˆ˜ì§‘ ì¤‘: ë„¤ì´ë²„ ë‰´ìŠ¤ - {keyword}")
                        progress_bar.progress(current_step / total_steps)

                        news_items = search_naver_news(keyword, max_news)
                        if news_items:
                            all_news.extend(news_items)
                            st.success(f"âœ… ë„¤ì´ë²„ '{keyword}': {len(news_items)}ê±´")

                            if use_gpt:
                                summary = summarize_korean_news(news_items, keyword)
                                if summary:
                                    summaries[f"ë„¤ì´ë²„-{keyword}"] = summary

                        time.sleep(0.3)
                else:
                    # ì›¹ ìŠ¤í¬ë˜í•‘
                    current_step += 1
                    status_text.text(f"ìˆ˜ì§‘ ì¤‘: {source_name}")
                    progress_bar.progress(current_step / total_steps)

                    news_items = scrape_news_site(source_name, max_news)
                    if news_items:
                        all_news.extend(news_items)
                        st.success(f"âœ… {source_name}: {len(news_items)}ê±´")

                        # ì™¸ì‹ ì€ ë²ˆì—­/ìš”ì•½
                        source_info = NEWS_SOURCES.get(source_name, {})
                        if use_gpt and source_info.get('language') == 'en':
                            summary = translate_and_summarize(news_items, source_name)
                            if summary:
                                summaries[source_name] = summary

                    time.sleep(0.5)

            progress_bar.progress(1.0)
            status_text.text("ì™„ë£Œ!")

            # ê²°ê³¼ ì €ì¥
            st.session_state['news_results'] = all_news
            st.session_state['news_summaries'] = summaries

            st.success(f"âœ… ì´ {len(all_news)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

            # í…”ë ˆê·¸ë¨ ë°œì†¡
            if send_to_telegram and all_news:
                msg = f"ğŸ“° <b>ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼</b>\n"
                msg += f"ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"

                # ì†ŒìŠ¤ë³„ ìš”ì•½
                for source_key, summary in list(summaries.items())[:3]:
                    msg += f"<b>[{source_key}]</b>\n"
                    # ìš”ì•½ ì²« 200ìë§Œ
                    msg += f"{summary[:200]}...\n\n"

                if send_telegram(msg):
                    st.success("âœ… í…”ë ˆê·¸ë¨ ë°œì†¡ ì™„ë£Œ")
                else:
                    st.warning("âš ï¸ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨")

        # ê²°ê³¼ í‘œì‹œ
        if 'news_results' in st.session_state and st.session_state['news_results']:
            all_news = st.session_state['news_results']
            summaries = st.session_state.get('news_summaries', {})

            st.markdown("---")
            st.subheader(f"ğŸ“‹ ìˆ˜ì§‘ ê²°ê³¼ ({len(all_news)}ê±´)")

            # GPT ìš”ì•½/ë²ˆì—­ í‘œì‹œ
            if summaries:
                st.markdown("### ğŸ“ GPT ìš”ì•½/ë²ˆì—­")
                for source_key, summary in summaries.items():
                    with st.expander(f"**{source_key}**", expanded=False):
                        st.markdown(summary)

            st.markdown("### ğŸ“° ë‰´ìŠ¤ ëª©ë¡")

            # ì†ŒìŠ¤ í•„í„°
            sources_found = list(set(n.get('source', 'ê¸°íƒ€') for n in all_news))
            selected_filter = st.selectbox("ì†ŒìŠ¤ í•„í„°", ["ì „ì²´"] + sources_found)

            if selected_filter == "ì „ì²´":
                filtered_news = all_news
            else:
                filtered_news = [n for n in all_news if n.get('source') == selected_filter]

            # í…Œì´ë¸” í‘œì‹œ
            if filtered_news:
                df_data = []
                for n in filtered_news:
                    df_data.append({
                        'ì†ŒìŠ¤': n.get('source', ''),
                        'ì œëª©': n.get('title', ''),
                        'ì–¸ì–´': 'ğŸ‡°ğŸ‡·' if n.get('language') == 'ko' else 'ğŸ‡ºğŸ‡¸',
                        'ë§í¬': n.get('link', '')
                    })

                df = pd.DataFrame(df_data)
                st.dataframe(df[['ì†ŒìŠ¤', 'ì œëª©', 'ì–¸ì–´']], use_container_width=True)

                # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
                output = BytesIO()
                df.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)

                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=output,
                    file_name=f"news_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

    with tab2:
        st.subheader("âš™ï¸ ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡")

        for group_name, sources in SOURCE_GROUPS.items():
            with st.expander(f"**{group_name}** ({len(sources)}ê°œ)", expanded=False):
                for source_name in sources:
                    source_info = NEWS_SOURCES.get(source_name, {})
                    lang_flag = "ğŸ‡°ğŸ‡·" if source_info.get('language') == 'ko' else "ğŸ‡ºğŸ‡¸"
                    url = source_info.get('url', '')
                    desc = source_info.get('description', '')

                    st.markdown(f"**{lang_flag} {source_name}**")
                    st.caption(f"{desc}")
                    if url:
                        st.caption(f"ğŸ”— {url}")
                    st.markdown("---")

        st.info("ğŸ’¡ ì†ŒìŠ¤ ì¶”ê°€/ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ í”¼ë“œë°± í˜ì´ì§€ì—ì„œ ìš”ì²­í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
