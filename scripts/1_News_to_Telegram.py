# coding=utf-8
"""
================================================================================
1_News_to_Telegram.py
================================================================================
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° í…”ë ˆê·¸ë¨ ì „ì†¡ ìŠ¤í¬ë¦½íŠ¸

[ê¸°ëŠ¥]
- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í†µí•œ ë‰´ìŠ¤ ìˆ˜ì§‘
- í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ê²€ìƒ‰ (ë³µìˆ˜ í‚¤ì›Œë“œ ì§€ì›)
- ì–¸ë¡ ì‚¬ í•„í„°ë§ (ì„ íƒì )
- HTML íƒœê·¸ ì œê±° ë° ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°
- ì—‘ì…€ ì €ì¥ ë° í…”ë ˆê·¸ë¨ ì „ì†¡

[í™˜ê²½ë³€ìˆ˜] (.env íŒŒì¼ì— ì„¤ì •)
- NAVER_CLIENT_ID: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID
- NAVER_CLIENT_SECRET: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ì‹œí¬ë¦¿
- BOT_TOKEN: í…”ë ˆê·¸ë¨ ë´‡ í† í°
- CHAT_ID: í…”ë ˆê·¸ë¨ ì±„íŒ… ID

[ì¶œë ¥]
- output/naver_news_YYYYMMDD.xlsx

[ì‹¤í–‰ ë°©ë²•]
$ python scripts/1_News_to_Telegram.py

[ì‹¤í–‰ ì˜µì…˜]
- ê¸°ë³¸ ì‹¤í–‰: ë°˜ë„ì²´, ì‹¤ì  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
- í…”ë ˆê·¸ë¨ ì „ì†¡ í¬í•¨: --telegram ì˜µì…˜ ì¶”ê°€

[Streamlit ì—°ë™]
- search_news(): í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
- save_to_excel(): ì—‘ì…€ ì €ì¥
- send_to_telegram(): í…”ë ˆê·¸ë¨ ì „ì†¡
- ê° í•¨ìˆ˜ëŠ” ë…ë¦½ì ìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥

================================================================================
"""

import os
import re
import sys
import html
import requests
import warnings
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import quote

import pandas as pd
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼)
_script_dir = os.path.dirname(os.path.abspath(__file__))
_project_dir = os.path.dirname(_script_dir)
_env_path = os.path.join(_project_dir, '.env')
load_dotenv(dotenv_path=_env_path)

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')


# =============================================================================
# ìƒìˆ˜ ì •ì˜
# =============================================================================

# ì£¼ìš” ì–¸ë¡ ì‚¬ ëª©ë¡ (Streamlitì—ì„œ ì„ íƒ ê°€ëŠ¥)
PRESS_LIST = {
    'ì „ì²´': None,
    'ì¡°ì„ ì¼ë³´': 'ì¡°ì„ ì¼ë³´',
    'ì¤‘ì•™ì¼ë³´': 'ì¤‘ì•™ì¼ë³´',
    'ë™ì•„ì¼ë³´': 'ë™ì•„ì¼ë³´',
    'í•œêµ­ê²½ì œ': 'í•œêµ­ê²½ì œ',
    'ë§¤ì¼ê²½ì œ': 'ë§¤ì¼ê²½ì œ',
    'ì„œìš¸ê²½ì œ': 'ì„œìš¸ê²½ì œ',
    'íŒŒì´ë‚¸ì…œë‰´ìŠ¤': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
    'ë¨¸ë‹ˆíˆ¬ë°ì´': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
    'ì´ë°ì¼ë¦¬': 'ì´ë°ì¼ë¦¬',
    'ì—°í•©ë‰´ìŠ¤': 'ì—°í•©ë‰´ìŠ¤',
    'YTN': 'YTN',
    'SBS': 'SBS',
    'KBS': 'KBS',
    'MBC': 'MBC',
}

# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ì„¤ì •
NAVER_NEWS_API_URL = "https://openapi.naver.com/v1/search/news.json"


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def clean_html_tags(text: str) -> str:
    """
    HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""

    # HTML ì—”í‹°í‹° ë””ì½”ë”© (&lt; -> <, &amp; -> & ë“±)
    text = html.unescape(text)

    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)

    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def extract_press_name(original_link: str, title: str) -> str:
    """
    ë‰´ìŠ¤ ë§í¬ ë˜ëŠ” ì œëª©ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ

    Args:
        original_link: ì›ë³¸ ë‰´ìŠ¤ ë§í¬
        title: ë‰´ìŠ¤ ì œëª©

    Returns:
        ì–¸ë¡ ì‚¬ëª… (ì¶”ì¶œ ì‹¤íŒ¨ì‹œ 'ê¸°íƒ€')
    """
    # ë„ë©”ì¸ì—ì„œ ì–¸ë¡ ì‚¬ ì¶”ì¶œ
    domain_press_map = {
        'chosun.com': 'ì¡°ì„ ì¼ë³´',
        'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
        'donga.com': 'ë™ì•„ì¼ë³´',
        'hankyung.com': 'í•œêµ­ê²½ì œ',
        'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
        'sedaily.com': 'ì„œìš¸ê²½ì œ',
        'fnnews.com': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
        'edaily.co.kr': 'ì´ë°ì¼ë¦¬',
        'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤',
        'ytn.co.kr': 'YTN',
        'sbs.co.kr': 'SBS',
        'kbs.co.kr': 'KBS',
        'mbc.co.kr': 'MBC',
        'news1.kr': 'ë‰´ìŠ¤1',
        'newsis.com': 'ë‰´ì‹œìŠ¤',
        'etnews.com': 'ì „ìì‹ ë¬¸',
        'zdnet.co.kr': 'ZDNet',
        'bloter.net': 'ë¸”ë¡œí„°',
    }

    for domain, press in domain_press_map.items():
        if domain in original_link:
            return press

    return 'ê¸°íƒ€'


def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (ì œëª© ê¸°ì¤€)

    Args:
        df: ë‰´ìŠ¤ DataFrame

    Returns:
        ì¤‘ë³µ ì œê±°ëœ DataFrame
    """
    if df.empty:
        return df

    # ì œëª©ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë¹„êµìš© ì»¬ëŸ¼ ìƒì„±
    df['title_clean'] = df['title'].apply(lambda x: re.sub(r'[^\w\s]', '', str(x).lower()))

    # ì¤‘ë³µ ì œê±°
    df_unique = df.drop_duplicates(subset=['title_clean'], keep='first')

    # ë¹„êµìš© ì»¬ëŸ¼ ì‚­ì œ
    df_unique = df_unique.drop(columns=['title_clean'])

    return df_unique.reset_index(drop=True)


# =============================================================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API
# =============================================================================

def search_news(
    keywords: List[str],
    max_results: int = 10,
    press_filter: Optional[List[str]] = None,
    search_date: Optional[str] = None
) -> pd.DataFrame:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰

    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        max_results: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        press_filter: ì–¸ë¡ ì‚¬ í•„í„° ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        search_date: ê²€ìƒ‰ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: ì˜¤ëŠ˜)

    Returns:
        ë‰´ìŠ¤ DataFrame (columns: date, keyword, press, title, summary, original_url)
    """
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')

    if not client_id or not client_secret:
        print("[ì˜¤ë¥˜] NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRETì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    headers = {
        'X-Naver-Client-Id': client_id,
        'X-Naver-Client-Secret': client_secret
    }

    if search_date is None:
        search_date = datetime.now().strftime('%Y-%m-%d')

    all_news = []

    for keyword in keywords:
        print(f"  - '{keyword}' ê²€ìƒ‰ ì¤‘...")

        params = {
            'query': keyword,
            'display': min(max_results, 100),  # ìµœëŒ€ 100ê°œ
            'start': 1,
            'sort': 'date'  # ìµœì‹ ìˆœ ì •ë ¬
        }

        try:
            response = requests.get(
                NAVER_NEWS_API_URL,
                headers=headers,
                params=params,
                timeout=10
            )
            response.raise_for_status()

            data = response.json()
            items = data.get('items', [])

            for item in items:
                # HTML íƒœê·¸ ì œê±°
                title = clean_html_tags(item.get('title', ''))
                description = clean_html_tags(item.get('description', ''))
                original_url = item.get('originallink', item.get('link', ''))
                pub_date = item.get('pubDate', '')

                # ë‚ ì§œ íŒŒì‹± (RFC 2822 í˜•ì‹)
                try:
                    dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')
                    news_date = dt.strftime('%Y-%m-%d')
                except:
                    news_date = search_date

                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                press = extract_press_name(original_url, title)

                # ì–¸ë¡ ì‚¬ í•„í„°ë§
                if press_filter and 'ì „ì²´' not in press_filter:
                    if press not in press_filter:
                        continue

                all_news.append({
                    'date': news_date,
                    'keyword': keyword,
                    'press': press,
                    'title': title,
                    'summary': description,
                    'original_url': original_url
                })

            print(f"    -> {len(items)}ê±´ ìˆ˜ì§‘")

        except requests.exceptions.RequestException as e:
            print(f"    [ì˜¤ë¥˜] API ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

    # DataFrame ìƒì„±
    df = pd.DataFrame(all_news)

    if not df.empty:
        # ì¤‘ë³µ ì œê±°
        original_count = len(df)
        df = remove_duplicates(df)
        removed_count = original_count - len(df)
        if removed_count > 0:
            print(f"  -> ì¤‘ë³µ {removed_count}ê±´ ì œê±°")

    return df


# =============================================================================
# LLM ìš”ì•½ í•¨ìˆ˜ (ì¶”í›„ í™•ì¥ìš©)
# =============================================================================

def generate_summary_llm(text: str) -> str:
    """
    LLMì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ìš”ì•½ (ì¶”í›„ êµ¬í˜„)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    # TODO: OpenAI APIë¥¼ ì‚¬ìš©í•œ ìš”ì•½ êµ¬í˜„
    # í˜„ì¬ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
    return text


def enhance_summaries_with_llm(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameì˜ summaryë¥¼ LLMìœ¼ë¡œ ê°œì„  (ì¶”í›„ êµ¬í˜„)

    Args:
        df: ë‰´ìŠ¤ DataFrame

    Returns:
        ìš”ì•½ì´ ê°œì„ ëœ DataFrame
    """
    # TODO: LLM ìš”ì•½ ì ìš©
    # í˜„ì¬ëŠ” ì›ë³¸ DataFrame ë°˜í™˜
    return df


# =============================================================================
# ì—‘ì…€ ì €ì¥
# =============================================================================

def save_to_excel(
    df: pd.DataFrame,
    output_dir: Optional[str] = None,
    filename: Optional[str] = None
) -> str:
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥

    Args:
        df: ë‰´ìŠ¤ DataFrame
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: í”„ë¡œì íŠ¸/output)
        filename: íŒŒì¼ëª… (ê¸°ë³¸: naver_news_YYYYMMDD.xlsx)

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir is None:
        output_dir = os.path.join(_project_dir, 'output')

    os.makedirs(output_dir, exist_ok=True)

    if filename is None:
        today_str = datetime.now().strftime('%Y%m%d')
        filename = f"naver_news_{today_str}.xlsx"

    filepath = os.path.join(output_dir, filename)

    # ì—‘ì…€ ì €ì¥
    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
        # ì „ì²´ ë‰´ìŠ¤
        df.to_excel(writer, sheet_name='All_News', index=False)

        # í‚¤ì›Œë“œë³„ ì‹œíŠ¸ ìƒì„±
        if 'keyword' in df.columns:
            for keyword in df['keyword'].unique():
                df_keyword = df[df['keyword'] == keyword]
                # ì‹œíŠ¸ëª…ì€ 31ì ì œí•œ, íŠ¹ìˆ˜ë¬¸ì ì œê±°
                sheet_name = re.sub(r'[\\/*?:\[\]]', '', keyword)[:31]
                df_keyword.to_excel(writer, sheet_name=sheet_name, index=False)

    print(f"[ì €ì¥ ì™„ë£Œ] {filepath}")
    return filepath


# =============================================================================
# í…”ë ˆê·¸ë¨ ì „ì†¡
# =============================================================================

def send_to_telegram(
    message: str,
    bot_token: Optional[str] = None,
    chat_id: Optional[str] = None
) -> bool:
    """
    í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡

    Args:
        message: ì „ì†¡í•  ë©”ì‹œì§€
        bot_token: ë´‡ í† í° (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜)
        chat_id: ì±„íŒ… ID (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜)

    Returns:
        ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    if bot_token is None:
        bot_token = os.getenv('BOT_TOKEN')
    if chat_id is None:
        chat_id = os.getenv('CHAT_ID')

    if not bot_token or not chat_id:
        print("[ì˜¤ë¥˜] BOT_TOKEN ë˜ëŠ” CHAT_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    # ê³µë°± ì œê±°
    bot_token = bot_token.strip()
    chat_id = chat_id.strip()

    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

    payload = {
        'chat_id': chat_id,
        'text': message,
        'parse_mode': 'HTML',
        'disable_web_page_preview': True
    }

    try:
        response = requests.post(url, json=payload, timeout=10)
        response.raise_for_status()

        result = response.json()
        if result.get('ok'):
            return True
        else:
            print(f"[ì˜¤ë¥˜] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {result.get('description')}")
            return False

    except requests.exceptions.RequestException as e:
        print(f"[ì˜¤ë¥˜] í…”ë ˆê·¸ë¨ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return False


def send_news_to_telegram(
    df: pd.DataFrame,
    max_news: int = 5,
    keywords_to_send: Optional[List[str]] = None
) -> int:
    """
    ë‰´ìŠ¤ DataFrameì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡

    Args:
        df: ë‰´ìŠ¤ DataFrame
        max_news: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì „ì†¡ ë‰´ìŠ¤ ìˆ˜
        keywords_to_send: ì „ì†¡í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)

    Returns:
        ì „ì†¡ëœ ë‰´ìŠ¤ ìˆ˜
    """
    if df.empty:
        print("[ì•Œë¦¼] ì „ì†¡í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    sent_count = 0

    # í‚¤ì›Œë“œ í•„í„°ë§
    if keywords_to_send:
        df = df[df['keyword'].isin(keywords_to_send)]

    # í‚¤ì›Œë“œë³„ë¡œ ë©”ì‹œì§€ êµ¬ì„±
    for keyword in df['keyword'].unique():
        df_keyword = df[df['keyword'] == keyword].head(max_news)

        # ë©”ì‹œì§€ êµ¬ì„±
        message_lines = [f"<b>ğŸ“° [{keyword}] ë‰´ìŠ¤ ({len(df_keyword)}ê±´)</b>\n"]

        for idx, row in df_keyword.iterrows():
            news_line = f"â€¢ <a href='{row['original_url']}'>{row['title']}</a>"
            if row.get('press'):
                news_line += f" ({row['press']})"
            message_lines.append(news_line)

        message = '\n'.join(message_lines)

        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (4096ì)
        if len(message) > 4000:
            message = message[:4000] + "\n..."

        if send_to_telegram(message):
            sent_count += len(df_keyword)
            print(f"  -> '{keyword}' ë‰´ìŠ¤ {len(df_keyword)}ê±´ ì „ì†¡ ì™„ë£Œ")

    return sent_count


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main(send_telegram: bool = False):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

    Args:
        send_telegram: í…”ë ˆê·¸ë¨ ì „ì†¡ ì—¬ë¶€
    """
    print("=" * 60)
    print("  ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° í…”ë ˆê·¸ë¨ ì „ì†¡")
    print(f"  ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print()

    # ê²€ìƒ‰ ì„¤ì • (ì˜ˆì‹œ: ë°˜ë„ì²´, ì‹¤ì )
    keywords = ['ë°˜ë„ì²´', 'ì‹¤ì ']
    max_results = 10
    press_filter = None  # ì „ì²´ ì–¸ë¡ ì‚¬

    print(f"[ì„¤ì •]")
    print(f"  - í‚¤ì›Œë“œ: {keywords}")
    print(f"  - í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬: {max_results}ê°œ")
    print(f"  - ì–¸ë¡ ì‚¬ í•„í„°: {'ì „ì²´' if not press_filter else press_filter}")
    print()

    # ë‰´ìŠ¤ ê²€ìƒ‰
    print("[1/3] ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    df = search_news(
        keywords=keywords,
        max_results=max_results,
        press_filter=press_filter
    )

    if df.empty:
        print("[ì™„ë£Œ] ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"  -> ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    print()

    # ì—‘ì…€ ì €ì¥
    print("[2/3] ì—‘ì…€ ì €ì¥ ì¤‘...")
    filepath = save_to_excel(df)
    print()

    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    print("[ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°]")
    print("-" * 60)
    for keyword in df['keyword'].unique():
        df_keyword = df[df['keyword'] == keyword]
        print(f"\n[{keyword}] - {len(df_keyword)}ê±´")
        for idx, row in df_keyword.head(3).iterrows():
            print(f"  - {row['title'][:50]}...")
            print(f"    {row['press']} | {row['date']}")
    print()

    # í…”ë ˆê·¸ë¨ ì „ì†¡ (ì˜µì…˜)
    if send_telegram:
        print("[3/3] í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘...")
        sent = send_news_to_telegram(df, max_news=5)
        print(f"  -> ì´ {sent}ê±´ ì „ì†¡ ì™„ë£Œ")
    else:
        print("[3/3] í…”ë ˆê·¸ë¨ ì „ì†¡: ê±´ë„ˆëœ€ (--telegram ì˜µì…˜ìœ¼ë¡œ í™œì„±í™”)")

    print()
    print("=" * 60)
    print("[ì™„ë£Œ] ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)

    return filepath


if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    send_telegram = '--telegram' in sys.argv
    main(send_telegram=send_telegram)
