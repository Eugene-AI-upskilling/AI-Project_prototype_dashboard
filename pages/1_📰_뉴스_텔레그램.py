# coding=utf-8
"""
í˜ì´ì§€ 1: ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨ - ë‹¤ì–‘í•œ ì†ŒìŠ¤ + í‚¤ì›Œë“œ ê²€ìƒ‰
"""

import streamlit as st
import pandas as pd
import os
import time
from datetime import datetime
from typing import List, Dict
from io import BytesIO

import requests
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

st.set_page_config(page_title="ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨", page_icon="ğŸ“°", layout="wide")


# =============================================================================
# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜ (ì‘ë™ í™•ì¸ëœ ì‚¬ì´íŠ¸ë§Œ)
# =============================================================================

NEWS_SOURCES = {
    # êµ­ë‚´ ë°”ì´ì˜¤
    "ë”ë°”ì´ì˜¤": {
        "url": "https://www.thebionews.net/",
        "description": "êµ­ë‚´ ë°”ì´ì˜¤ ì „ë¬¸ ë‰´ìŠ¤",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤"
    },
    "íˆíŠ¸ë‰´ìŠ¤": {
        "url": "https://www.hitnews.co.kr/",
        "description": "êµ­ë‚´ í—¬ìŠ¤ì¼€ì–´/ì œì•½ ë‰´ìŠ¤",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤"
    },
    "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸": {
        "url": "https://www.hankyung.com/bioinsight",
        "description": "í•œêµ­ê²½ì œ ë°”ì´ì˜¤ ì„¹ì…˜",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤"
    },
    # ì™¸ì‹  ë°”ì´ì˜¤
    "FierceBiotech": {
        "url": "https://www.fiercebiotech.com/",
        "description": "ë°”ì´ì˜¤í… ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ì™¸ì‹  ë°”ì´ì˜¤"
    },
    "FiercePharma": {
        "url": "https://www.fiercepharma.com/",
        "description": "ì œì•½ ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ì™¸ì‹  ë°”ì´ì˜¤"
    },
    # ì™¸ì‹  IT
    "TrendForce": {
        "url": "https://www.trendforce.com/",
        "description": "ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´ ì‹œì¥ ë¶„ì„",
        "language": "en",
        "category": "ì™¸ì‹  IT"
    },
    "The Register": {
        "url": "https://www.theregister.com/",
        "description": "IT/ì—”í„°í”„ë¼ì´ì¦ˆ ë‰´ìŠ¤",
        "language": "en",
        "category": "ì™¸ì‹  IT"
    },
}

SOURCE_GROUPS = {
    "ë„¤ì´ë²„ ë‰´ìŠ¤": ["ë„¤ì´ë²„ ë‰´ìŠ¤"],
    "êµ­ë‚´ ë°”ì´ì˜¤": ["ë”ë°”ì´ì˜¤", "íˆíŠ¸ë‰´ìŠ¤", "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸"],
    "ì™¸ì‹  ë°”ì´ì˜¤": ["FierceBiotech", "FiercePharma"],
    "ì™¸ì‹  IT": ["TrendForce", "The Register"],
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


# =============================================================================
# í—¬í¼ í•¨ìˆ˜
# =============================================================================

def get_secret(key, default=None):
    """Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
    try:
        value = st.secrets.get(key)
        if value:
            return value
    except Exception:
        pass
    value = os.environ.get(key)
    if value:
        return value
    return default


def keyword_match(title: str, keywords: List[str]) -> bool:
    """ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not keywords:
        return True
    title_lower = title.lower()
    for kw in keywords:
        if kw.lower() in title_lower:
            return True
    return False


# =============================================================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ API
# =============================================================================

def search_naver_news(keyword: str, display: int = 10) -> List[Dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ê²€ìƒ‰"""
    client_id = get_secret('NAVER_CLIENT_ID')
    client_secret = get_secret('NAVER_CLIENT_SECRET')

    if not client_id or not client_secret:
        st.error("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {"query": keyword, "display": display, "sort": "date"}

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = []
        for item in data.get('items', []):
            title = item['title'].replace('<b>', '').replace('</b>', '')
            results.append({
                'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
                'keyword': keyword,
                'title': title,
                'link': item['originallink'] or item['link'],
                'pubDate': item['pubDate'],
                'language': 'ko'
            })
        return results
    except Exception as e:
        st.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# =============================================================================
# ì›¹ ìŠ¤í¬ë˜í•‘
# =============================================================================

def scrape_thebionews(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """ë”ë°”ì´ì˜¤ ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.thebionews.net/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for article in soup.select('div.td-module-container, article, .entry-title')[:max_items * 2]:
            a = article.find('a')
            if a:
                title = a.get_text(strip=True)
                link = a.get('href', '')
                if title and len(title) > 10 and keyword_match(title, keywords):
                    if not link.startswith('http'):
                        link = 'https://www.thebionews.net' + link
                    results.append({
                        'source': 'ë”ë°”ì´ì˜¤',
                        'title': title,
                        'link': link,
                        'language': 'ko'
                    })
                    if len(results) >= max_items:
                        break
    except Exception as e:
        st.warning(f"ë”ë°”ì´ì˜¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_hitnews(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """íˆíŠ¸ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.hitnews.co.kr/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for a in soup.select('a'):
            href = a.get('href', '')
            if '/news/articleView' in href:
                title = a.get_text(strip=True)
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.hitnews.co.kr' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'íˆíŠ¸ë‰´ìŠ¤',
                            'title': title,
                            'link': href,
                            'language': 'ko'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"íˆíŠ¸ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_hankyung_bio(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.hankyung.com/bioinsight', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for a in soup.select('a'):
            href = a.get('href', '')
            if '/article/' in href:
                title = a.get_text(strip=True)
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.hankyung.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸',
                            'title': title,
                            'link': href,
                            'language': 'ko'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_fiercebiotech(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """FierceBiotech ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.fiercebiotech.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for h in soup.select('h2, h3'):
            a = h.find('a')
            if a:
                title = a.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.fiercebiotech.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'FierceBiotech',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"FierceBiotech ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_fiercepharma(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """FiercePharma ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.fiercepharma.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for h in soup.select('h2, h3'):
            a = h.find('a')
            if a:
                title = a.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.fiercepharma.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'FiercePharma',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"FiercePharma ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_trendforce(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """TrendForce ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.trendforce.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for article in soup.select('article, div.post, .news-item'):
            a = article.find('a')
            title_elem = article.find(['h2', 'h3', 'h4'])
            if a and title_elem:
                title = title_elem.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 10 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.trendforce.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'TrendForce',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"TrendForce ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_theregister(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """The Register ìŠ¤í¬ë˜í•‘"""
    results = []
    try:
        r = requests.get('https://www.theregister.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for article in soup.select('article'):
            a = article.find('a')
            title_elem = article.find(['h2', 'h3', 'h4'])
            if a and title_elem:
                title = title_elem.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.theregister.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'The Register',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"The Register ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


SCRAPER_MAP = {
    "ë”ë°”ì´ì˜¤": scrape_thebionews,
    "íˆíŠ¸ë‰´ìŠ¤": scrape_hitnews,
    "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸": scrape_hankyung_bio,
    "FierceBiotech": scrape_fiercebiotech,
    "FiercePharma": scrape_fiercepharma,
    "TrendForce": scrape_trendforce,
    "The Register": scrape_theregister,
}


# =============================================================================
# GPT ìš”ì•½/ë²ˆì—­
# =============================================================================

def translate_and_summarize(news_items: List[Dict], source_name: str) -> str:
    """ì™¸ì‹  ë‰´ìŠ¤ ë²ˆì—­ ë° ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')
    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:7]])

        prompt = f"""ë‹¤ìŒì€ '{source_name}'ì˜ ìµœì‹  í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

[ìš”êµ¬ì‚¬í•­]
- ê° í—¤ë“œë¼ì¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­
- ì „ì²´ì ì¸ íŠ¸ë Œë“œ/í•µì‹¬ ì´ìŠˆë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- íˆ¬ìì/ì—…ê³„ ê´€ê³„ì ê´€ì ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ ê°•ì¡°

[í—¤ë“œë¼ì¸]
{news_text}

[ì¶œë ¥ í˜•ì‹]
## ì£¼ìš” í—¤ë“œë¼ì¸ (ë²ˆì—­)
- (ë²ˆì—­1)
- (ë²ˆì—­2)
...

## í•µì‹¬ ìš”ì•½
(2-3ë¬¸ì¥ ìš”ì•½)
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ/ë°”ì´ì˜¤/IT ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ë²ˆì—­/ìš”ì•½ ì‹¤íŒ¨: {str(e)}"


def summarize_korean_news(news_items: List[Dict], source_name: str) -> str:
    """êµ­ë‚´ ë‰´ìŠ¤ ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')
    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:5]])

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³  í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": f"ì†ŒìŠ¤: {source_name}\n\në‰´ìŠ¤:\n{news_text}"}
            ],
            temperature=0.3,
            max_tokens=200
        )
        return response.choices[0].message.content
    except Exception:
        return None


def send_telegram(message: str) -> bool:
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    bot_token = get_secret('BOT_TOKEN')
    chat_id = get_secret('CHAT_ID')

    if not bot_token or not chat_id:
        return False

    url = f"https://api.telegram.org/bot{bot_token.strip()}/sendMessage"
    try:
        response = requests.post(url, data={
            'chat_id': chat_id.strip(),
            'text': message[:4096],
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }, timeout=30)
        return response.status_code == 200
    except:
        return False


# =============================================================================
# ë©”ì¸ ì•±
# =============================================================================

def main():
    st.title("ğŸ“° ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨")
    st.markdown("ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ â†’ GPT ìš”ì•½/ë²ˆì—­ â†’ í…”ë ˆê·¸ë¨ ë°œì†¡")

    st.markdown("---")

    # íƒ­
    tab1, tab2 = st.tabs(["ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘", "âš™ï¸ ì†ŒìŠ¤ ëª©ë¡"])

    with tab1:
        # ì†ŒìŠ¤ ê·¸ë£¹ ì„ íƒ
        st.subheader("ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ")

        selected_groups = st.multiselect(
            "ì†ŒìŠ¤ ê·¸ë£¹",
            list(SOURCE_GROUPS.keys()),
            default=["ë„¤ì´ë²„ ë‰´ìŠ¤"]
        )

        # ì„ íƒëœ ì†ŒìŠ¤ ëª©ë¡
        selected_sources = []
        for group in selected_groups:
            selected_sources.extend(SOURCE_GROUPS[group])

        if selected_sources:
            st.info(f"ì„ íƒëœ ì†ŒìŠ¤: {', '.join(selected_sources)}")

        st.markdown("---")

        # í‚¤ì›Œë“œ ì…ë ¥
        st.subheader("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
        keywords_input = st.text_area(
            "ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
            value="ë°˜ë„ì²´\nAI\në°”ì´ì˜¤",
            height=100,
            help="í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ëœ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ìˆ˜ì§‘."
        )

        keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

        st.markdown("---")

        # ì˜µì…˜
        st.subheader("âš™ï¸ ì˜µì…˜")
        col1, col2, col3 = st.columns(3)

        with col1:
            max_news = st.slider("ì†ŒìŠ¤ë‹¹ ë‰´ìŠ¤ ìˆ˜", 5, 30, 10)
        with col2:
            use_gpt = st.checkbox("GPT ìš”ì•½/ë²ˆì—­", value=True)
        with col3:
            send_to_telegram = st.checkbox("í…”ë ˆê·¸ë¨ ë°œì†¡", value=False)

        st.markdown("---")

        # ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True):
            if not selected_sources:
                st.error("ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return

            all_news = []
            summaries = {}

            progress_bar = st.progress(0)
            status_text = st.empty()

            total_steps = len(selected_sources)
            current_step = 0

            for source_name in selected_sources:
                current_step += 1
                status_text.text(f"ìˆ˜ì§‘ ì¤‘: {source_name}")
                progress_bar.progress(current_step / total_steps)

                if source_name == "ë„¤ì´ë²„ ë‰´ìŠ¤":
                    # ë„¤ì´ë²„ëŠ” í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                    for kw in keywords:
                        news_items = search_naver_news(kw, max_news)
                        if news_items:
                            all_news.extend(news_items)
                            st.success(f"âœ… ë„¤ì´ë²„ '{kw}': {len(news_items)}ê±´")

                            if use_gpt and news_items:
                                summary = summarize_korean_news(news_items, f"ë„¤ì´ë²„-{kw}")
                                if summary:
                                    summaries[f"ë„¤ì´ë²„-{kw}"] = summary
                        time.sleep(0.3)
                else:
                    # ì›¹ ìŠ¤í¬ë˜í•‘ (í‚¤ì›Œë“œ í•„í„°ë§)
                    scraper = SCRAPER_MAP.get(source_name)
                    if scraper:
                        news_items = scraper(keywords, max_news)
                        if news_items:
                            all_news.extend(news_items)
                            st.success(f"âœ… {source_name}: {len(news_items)}ê±´")

                            # GPT ìš”ì•½/ë²ˆì—­
                            source_info = NEWS_SOURCES.get(source_name, {})
                            if use_gpt:
                                if source_info.get('language') == 'en':
                                    summary = translate_and_summarize(news_items, source_name)
                                else:
                                    summary = summarize_korean_news(news_items, source_name)
                                if summary:
                                    summaries[source_name] = summary

                time.sleep(0.5)

            progress_bar.progress(1.0)
            status_text.text("ì™„ë£Œ!")

            # ê²°ê³¼ ì €ì¥
            st.session_state['news_results'] = all_news
            st.session_state['news_summaries'] = summaries

            st.success(f"âœ… ì´ {len(all_news)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

            # í…”ë ˆê·¸ë¨ ë°œì†¡
            if send_to_telegram and all_news:
                msg = f"ğŸ“° <b>ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼</b>\n"
                msg += f"ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
                msg += f"í‚¤ì›Œë“œ: {', '.join(keywords)}\n\n"

                for source_key, summary in list(summaries.items())[:3]:
                    msg += f"<b>[{source_key}]</b>\n"
                    msg += f"{summary[:300]}...\n\n"

                if send_telegram(msg):
                    st.success("âœ… í…”ë ˆê·¸ë¨ ë°œì†¡ ì™„ë£Œ")
                else:
                    st.warning("âš ï¸ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨")

        # ê²°ê³¼ í‘œì‹œ
        if 'news_results' in st.session_state and st.session_state['news_results']:
            all_news = st.session_state['news_results']
            summaries = st.session_state.get('news_summaries', {})

            st.markdown("---")
            st.subheader(f"ğŸ“‹ ìˆ˜ì§‘ ê²°ê³¼ ({len(all_news)}ê±´)")

            # GPT ìš”ì•½/ë²ˆì—­
            if summaries:
                st.markdown("### ğŸ“ GPT ìš”ì•½/ë²ˆì—­")
                for source_key, summary in summaries.items():
                    with st.expander(f"**{source_key}**", expanded=False):
                        st.markdown(summary)

            st.markdown("### ğŸ“° ë‰´ìŠ¤ ëª©ë¡")

            # ì†ŒìŠ¤ í•„í„°
            sources_found = list(set(n.get('source', 'ê¸°íƒ€') for n in all_news))
            selected_filter = st.selectbox("ì†ŒìŠ¤ í•„í„°", ["ì „ì²´"] + sources_found)

            if selected_filter == "ì „ì²´":
                filtered_news = all_news
            else:
                filtered_news = [n for n in all_news if n.get('source') == selected_filter]

            # í…Œì´ë¸”
            if filtered_news:
                df_data = []
                for n in filtered_news:
                    df_data.append({
                        'ì†ŒìŠ¤': n.get('source', ''),
                        'ì œëª©': n.get('title', ''),
                        'ì–¸ì–´': 'ğŸ‡°ğŸ‡·' if n.get('language') == 'ko' else 'ğŸ‡ºğŸ‡¸',
                        'ë§í¬': n.get('link', '')
                    })

                df = pd.DataFrame(df_data)
                st.dataframe(df[['ì†ŒìŠ¤', 'ì œëª©', 'ì–¸ì–´']], use_container_width=True)

                # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
                output = BytesIO()
                df.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)

                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=output,
                    file_name=f"news_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

    with tab2:
        st.subheader("âš™ï¸ ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡")

        for group_name, sources in SOURCE_GROUPS.items():
            with st.expander(f"**{group_name}** ({len(sources)}ê°œ)", expanded=False):
                if group_name == "ë„¤ì´ë²„ ë‰´ìŠ¤":
                    st.markdown("**ğŸ‡°ğŸ‡· ë„¤ì´ë²„ ë‰´ìŠ¤**")
                    st.caption("ë„¤ì´ë²„ ë‰´ìŠ¤ API (í‚¤ì›Œë“œ ê²€ìƒ‰)")
                else:
                    for source_name in sources:
                        info = NEWS_SOURCES.get(source_name, {})
                        lang = "ğŸ‡°ğŸ‡·" if info.get('language') == 'ko' else "ğŸ‡ºğŸ‡¸"
                        st.markdown(f"**{lang} {source_name}**")
                        st.caption(f"{info.get('description', '')}")
                        st.caption(f"ğŸ”— {info.get('url', '')}")
                        st.markdown("---")

        st.info("ğŸ’¡ ì†ŒìŠ¤ ì¶”ê°€ ìš”ì²­ì€ í”¼ë“œë°± í˜ì´ì§€ì—ì„œ í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
