# coding=utf-8
"""
í˜ì´ì§€ 1: ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨ - ë‹¤ì–‘í•œ ì†ŒìŠ¤ + í‚¤ì›Œë“œ ê²€ìƒ‰
"""

import streamlit as st
import pandas as pd
import os
import time
import urllib.parse
from datetime import datetime, timedelta
from typing import List, Dict
from io import BytesIO

import requests
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

st.set_page_config(page_title="ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨", page_icon="ğŸ“°", layout="wide")


# =============================================================================
# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
# =============================================================================

NEWS_SOURCES = {
    "ë”ë°”ì´ì˜¤": {
        "url": "https://www.thebionews.net/",
        "search_url": "https://www.thebionews.net/news/articleList.html?sc_word={keyword}",
        "description": "êµ­ë‚´ ë°”ì´ì˜¤ ì „ë¬¸ ë‰´ìŠ¤",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤",
        "search_type": "url"
    },
    "íˆíŠ¸ë‰´ìŠ¤": {
        "url": "https://www.hitnews.co.kr/",
        "search_url": "https://www.hitnews.co.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word={keyword}",
        "description": "êµ­ë‚´ í—¬ìŠ¤ì¼€ì–´/ì œì•½ ë‰´ìŠ¤",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤",
        "search_type": "url"
    },
    "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸": {
        "url": "https://www.hankyung.com/bioinsight",
        "description": "í•œêµ­ê²½ì œ ë°”ì´ì˜¤ ì„¹ì…˜",
        "language": "ko",
        "category": "êµ­ë‚´ ë°”ì´ì˜¤",
        "search_type": "main"
    },
    "FierceBiotech": {
        "url": "https://www.fiercebiotech.com/",
        "description": "ë°”ì´ì˜¤í… ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ì™¸ì‹  ë°”ì´ì˜¤",
        "search_type": "main"
    },
    "FiercePharma": {
        "url": "https://www.fiercepharma.com/",
        "description": "ì œì•½ ì „ë¬¸ ì™¸ì‹ ",
        "language": "en",
        "category": "ì™¸ì‹  ë°”ì´ì˜¤",
        "search_type": "main"
    },
    "TrendForce": {
        "url": "https://www.trendforce.com/news/",
        "description": "ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´ ì‹œì¥ ë¶„ì„",
        "language": "en",
        "category": "ì™¸ì‹  IT",
        "search_type": "main"
    },
    "The Register": {
        "url": "https://www.theregister.com/",
        "search_url": "https://search.theregister.com/?q={keyword}",
        "description": "IT/ì—”í„°í”„ë¼ì´ì¦ˆ ë‰´ìŠ¤",
        "language": "en",
        "category": "ì™¸ì‹  IT",
        "search_type": "url"
    },
}

SOURCE_GROUPS = {
    "ë„¤ì´ë²„ ë‰´ìŠ¤": ["ë„¤ì´ë²„ ë‰´ìŠ¤"],
    "êµ­ë‚´ ë°”ì´ì˜¤": ["ë”ë°”ì´ì˜¤", "íˆíŠ¸ë‰´ìŠ¤", "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸"],
    "ì™¸ì‹  ë°”ì´ì˜¤": ["FierceBiotech", "FiercePharma"],
    "ì™¸ì‹  IT": ["TrendForce", "The Register"],
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


# =============================================================================
# í—¬í¼ í•¨ìˆ˜
# =============================================================================

def get_secret(key, default=None):
    """Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
    try:
        value = st.secrets.get(key)
        if value:
            return value
    except Exception:
        pass
    value = os.environ.get(key)
    if value:
        return value
    return default


def keyword_match(title: str, keywords: List[str]) -> bool:
    """ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not keywords:
        return True
    title_lower = title.lower()
    for kw in keywords:
        if kw.lower() in title_lower:
            return True
    return False


# =============================================================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ API
# =============================================================================

def search_naver_news(keyword: str, display: int = 10) -> List[Dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ê²€ìƒ‰"""
    client_id = get_secret('NAVER_CLIENT_ID')
    client_secret = get_secret('NAVER_CLIENT_SECRET')

    if not client_id or not client_secret:
        st.error("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {"query": keyword, "display": display, "sort": "date"}

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = []
        for item in data.get('items', []):
            title = item['title'].replace('<b>', '').replace('</b>', '')
            results.append({
                'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
                'keyword': keyword,
                'title': title,
                'link': item['originallink'] or item['link'],
                'pubDate': item['pubDate'],
                'language': 'ko'
            })
        return results
    except Exception as e:
        st.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# =============================================================================
# ì›¹ ìŠ¤í¬ë˜í•‘ - ê²€ìƒ‰ URL ì‚¬ìš©
# =============================================================================

def scrape_thebionews(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """ë”ë°”ì´ì˜¤ ìŠ¤í¬ë˜í•‘ (ê²€ìƒ‰ URL ì‚¬ìš©)"""
    results = []

    try:
        for keyword in keywords if keywords else ['']:
            if keyword:
                # ê²€ìƒ‰ URL ì‚¬ìš©
                encoded_kw = urllib.parse.quote(keyword)
                url = f"https://www.thebionews.net/news/articleList.html?sc_word={encoded_kw}"
            else:
                url = "https://www.thebionews.net/"

            r = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')

            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ íŒŒì‹±
            for article in soup.select('div.list-block, ul.type01 li, article, .table-row')[:max_items]:
                a = article.find('a')
                if a:
                    title = a.get_text(strip=True)
                    link = a.get('href', '')
                    if title and len(title) > 10:
                        if not link.startswith('http'):
                            link = 'https://www.thebionews.net' + link
                        if not any(r['link'] == link for r in results):
                            results.append({
                                'source': 'ë”ë°”ì´ì˜¤',
                                'keyword': keyword,
                                'title': title,
                                'link': link,
                                'language': 'ko'
                            })
                            if len(results) >= max_items:
                                break

            if len(results) >= max_items:
                break
            time.sleep(0.3)

    except Exception as e:
        st.warning(f"ë”ë°”ì´ì˜¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

    return results[:max_items]


def scrape_hitnews(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """íˆíŠ¸ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (ê²€ìƒ‰ URL ì‚¬ìš©)"""
    results = []

    try:
        for keyword in keywords if keywords else ['']:
            if keyword:
                # ê²€ìƒ‰ URL ì‚¬ìš©
                encoded_kw = urllib.parse.quote(keyword)
                url = f"https://www.hitnews.co.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word={encoded_kw}"
            else:
                url = "https://www.hitnews.co.kr/"

            r = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')

            for a in soup.select('a'):
                href = a.get('href', '')
                if '/news/articleView' in href:
                    title = a.get_text(strip=True)
                    if title and len(title) > 15:
                        if not href.startswith('http'):
                            href = 'https://www.hitnews.co.kr' + href
                        if not any(r['link'] == href for r in results):
                            results.append({
                                'source': 'íˆíŠ¸ë‰´ìŠ¤',
                                'keyword': keyword,
                                'title': title,
                                'link': href,
                                'language': 'ko'
                            })
                            if len(results) >= max_items:
                                break

            if len(results) >= max_items:
                break
            time.sleep(0.3)

    except Exception as e:
        st.warning(f"íˆíŠ¸ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

    return results[:max_items]


def scrape_hankyung_bio(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ (ë©”ì¸ í˜ì´ì§€)"""
    results = []
    try:
        r = requests.get('https://www.hankyung.com/bioinsight', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for a in soup.select('a'):
            href = a.get('href', '')
            if '/article/' in href:
                title = a.get_text(strip=True)
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.hankyung.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸',
                            'title': title,
                            'link': href,
                            'language': 'ko'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_fiercebiotech(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """FierceBiotech ìŠ¤í¬ë˜í•‘ (ë©”ì¸ í˜ì´ì§€)"""
    results = []
    try:
        r = requests.get('https://www.fiercebiotech.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for h in soup.select('h2, h3'):
            a = h.find('a')
            if a:
                title = a.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.fiercebiotech.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'FierceBiotech',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"FierceBiotech ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_fiercepharma(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """FiercePharma ìŠ¤í¬ë˜í•‘ (ë©”ì¸ í˜ì´ì§€)"""
    results = []
    try:
        r = requests.get('https://www.fiercepharma.com/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for h in soup.select('h2, h3'):
            a = h.find('a')
            if a:
                title = a.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 15 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.fiercepharma.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'FiercePharma',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"FiercePharma ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_trendforce(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """TrendForce ìŠ¤í¬ë˜í•‘ (ë‰´ìŠ¤ í˜ì´ì§€)"""
    results = []
    try:
        r = requests.get('https://www.trendforce.com/news/', headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        for article in soup.select('article, div.post, .news-item, div.list-item'):
            a = article.find('a')
            title_elem = article.find(['h2', 'h3', 'h4', 'a'])
            if a and title_elem:
                title = title_elem.get_text(strip=True)
                href = a.get('href', '')
                if title and len(title) > 10 and keyword_match(title, keywords):
                    if not href.startswith('http'):
                        href = 'https://www.trendforce.com' + href
                    if not any(r['link'] == href for r in results):
                        results.append({
                            'source': 'TrendForce',
                            'title': title,
                            'link': href,
                            'language': 'en'
                        })
                        if len(results) >= max_items:
                            break
    except Exception as e:
        st.warning(f"TrendForce ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    return results


def scrape_theregister(keywords: List[str], max_items: int = 20) -> List[Dict]:
    """The Register ìŠ¤í¬ë˜í•‘ (ê²€ìƒ‰ URL ì‚¬ìš©)"""
    results = []

    try:
        for keyword in keywords if keywords else ['']:
            if keyword:
                # ê²€ìƒ‰ URL ì‚¬ìš©
                encoded_kw = urllib.parse.quote(keyword)
                url = f"https://search.theregister.com/?q={encoded_kw}"
            else:
                url = "https://www.theregister.com/"

            r = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')

            # ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” ë©”ì¸ í˜ì´ì§€ íŒŒì‹±
            for article in soup.select('article, div.result, .search-result'):
                a = article.find('a')
                title_elem = article.find(['h2', 'h3', 'h4', 'a'])
                if a and title_elem:
                    title = title_elem.get_text(strip=True)
                    href = a.get('href', '')
                    if title and len(title) > 15:
                        if not href.startswith('http'):
                            href = 'https://www.theregister.com' + href
                        if not any(r['link'] == href for r in results):
                            results.append({
                                'source': 'The Register',
                                'keyword': keyword,
                                'title': title,
                                'link': href,
                                'language': 'en'
                            })
                            if len(results) >= max_items:
                                break

            if len(results) >= max_items:
                break
            time.sleep(0.3)

    except Exception as e:
        st.warning(f"The Register ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

    return results[:max_items]


SCRAPER_MAP = {
    "ë”ë°”ì´ì˜¤": scrape_thebionews,
    "íˆíŠ¸ë‰´ìŠ¤": scrape_hitnews,
    "í•œê²½ë°”ì´ì˜¤ì¸ì‚¬ì´íŠ¸": scrape_hankyung_bio,
    "FierceBiotech": scrape_fiercebiotech,
    "FiercePharma": scrape_fiercepharma,
    "TrendForce": scrape_trendforce,
    "The Register": scrape_theregister,
}


# =============================================================================
# GPT ìš”ì•½/ë²ˆì—­
# =============================================================================

def translate_and_summarize(news_items: List[Dict], source_name: str) -> str:
    """ì™¸ì‹  ë‰´ìŠ¤ ë²ˆì—­ ë° ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')
    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:7]])

        prompt = f"""ë‹¤ìŒì€ '{source_name}'ì˜ ìµœì‹  í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

[ìš”êµ¬ì‚¬í•­]
- ê° í—¤ë“œë¼ì¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­
- ì „ì²´ì ì¸ íŠ¸ë Œë“œ/í•µì‹¬ ì´ìŠˆë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- íˆ¬ìì/ì—…ê³„ ê´€ê³„ì ê´€ì ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ ê°•ì¡°

[í—¤ë“œë¼ì¸]
{news_text}

[ì¶œë ¥ í˜•ì‹]
## ì£¼ìš” í—¤ë“œë¼ì¸ (ë²ˆì—­)
- (ë²ˆì—­1)
- (ë²ˆì—­2)
...

## í•µì‹¬ ìš”ì•½
(2-3ë¬¸ì¥ ìš”ì•½)
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ/ë°”ì´ì˜¤/IT ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ë²ˆì—­/ìš”ì•½ ì‹¤íŒ¨: {str(e)}"


def summarize_korean_news(news_items: List[Dict], source_name: str) -> str:
    """êµ­ë‚´ ë‰´ìŠ¤ ìš”ì•½"""
    api_key = get_secret('OPENAI_API') or get_secret('OPENAI_API_KEY')
    if not api_key:
        return None

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)

        news_text = "\n".join([f"- {n['title']}" for n in news_items[:5]])

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³  í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": f"ì†ŒìŠ¤: {source_name}\n\në‰´ìŠ¤:\n{news_text}"}
            ],
            temperature=0.3,
            max_tokens=200
        )
        return response.choices[0].message.content
    except Exception:
        return None


def send_telegram(message: str) -> bool:
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    bot_token = get_secret('BOT_TOKEN')
    chat_id = get_secret('CHAT_ID')

    if not bot_token or not chat_id:
        return False

    url = f"https://api.telegram.org/bot{bot_token.strip()}/sendMessage"
    try:
        response = requests.post(url, data={
            'chat_id': chat_id.strip(),
            'text': message[:4096],
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }, timeout=30)
        return response.status_code == 200
    except:
        return False


# =============================================================================
# ë©”ì¸ ì•±
# =============================================================================

def main():
    st.title("ğŸ“° ë‰´ìŠ¤ â†’ í…”ë ˆê·¸ë¨")
    st.markdown("ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ â†’ GPT ìš”ì•½/ë²ˆì—­ â†’ í…”ë ˆê·¸ë¨ ë°œì†¡")

    st.markdown("---")

    # íƒ­
    tab1, tab2 = st.tabs(["ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘", "âš™ï¸ ì†ŒìŠ¤ ëª©ë¡"])

    with tab1:
        # ì†ŒìŠ¤ ê·¸ë£¹ ì„ íƒ
        st.subheader("ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ")

        selected_groups = st.multiselect(
            "ì†ŒìŠ¤ ê·¸ë£¹",
            list(SOURCE_GROUPS.keys()),
            default=["ë„¤ì´ë²„ ë‰´ìŠ¤"]
        )

        # ì„ íƒëœ ì†ŒìŠ¤ ëª©ë¡
        selected_sources = []
        for group in selected_groups:
            selected_sources.extend(SOURCE_GROUPS[group])

        if selected_sources:
            st.info(f"ì„ íƒëœ ì†ŒìŠ¤: {', '.join(selected_sources)}")

        st.markdown("---")

        # í‚¤ì›Œë“œ ì…ë ¥
        st.subheader("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")

        col_kw1, col_kw2 = st.columns([3, 1])

        with col_kw1:
            keywords_input = st.text_area(
                "ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                value="SKë°”ì´ì˜¤íŒœ\nì‚¼ì„±ë°”ì´ì˜¤\nì…€íŠ¸ë¦¬ì˜¨",
                height=100,
                help="í‚¤ì›Œë“œë¡œ ê° ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ê²€ìƒ‰í•©ë‹ˆë‹¤."
            )

        with col_kw2:
            st.markdown("<br>", unsafe_allow_html=True)
            fetch_all = st.checkbox("ì „ì²´ ìˆ˜ì§‘", value=False, help="í‚¤ì›Œë“œ ì—†ì´ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘")

        if fetch_all:
            keywords = []
            st.info("â„¹ï¸ ì „ì²´ ìˆ˜ì§‘ ëª¨ë“œ: ê° ì‚¬ì´íŠ¸ì˜ ìµœì‹  ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        else:
            keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

        st.markdown("---")

        # ê²€ìƒ‰ ê¸°ê°„ ì„¤ì •
        st.subheader("ğŸ“… ê²€ìƒ‰ ê¸°ê°„")
        col_date1, col_date2 = st.columns(2)

        with col_date1:
            period_option = st.selectbox(
                "ê¸°ê°„ ì„ íƒ",
                ["ì˜¤ëŠ˜", "ìµœê·¼ 3ì¼", "ìµœê·¼ 1ì£¼ì¼", "ìµœê·¼ 1ê°œì›”", "ì§ì ‘ ì…ë ¥"],
                index=2
            )

        with col_date2:
            if period_option == "ì§ì ‘ ì…ë ¥":
                date_range = st.date_input(
                    "ë‚ ì§œ ë²”ìœ„",
                    value=(datetime.now().date() - timedelta(days=7), datetime.now().date()),
                    max_value=datetime.now().date()
                )
            else:
                # ê¸°ê°„ ê³„ì‚° (í‘œì‹œìš©)
                if period_option == "ì˜¤ëŠ˜":
                    days = 1
                elif period_option == "ìµœê·¼ 3ì¼":
                    days = 3
                elif period_option == "ìµœê·¼ 1ì£¼ì¼":
                    days = 7
                else:  # ìµœê·¼ 1ê°œì›”
                    days = 30
                st.info(f"ğŸ“† {(datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')} ~ {datetime.now().strftime('%Y-%m-%d')}")

        st.markdown("---")

        # ì˜µì…˜
        st.subheader("âš™ï¸ ì˜µì…˜")
        col1, col2, col3 = st.columns(3)

        with col1:
            max_news = st.slider("ì†ŒìŠ¤ë‹¹ ë‰´ìŠ¤ ìˆ˜", 5, 30, 10)
        with col2:
            use_gpt = st.checkbox("GPT ìš”ì•½/ë²ˆì—­", value=True)
        with col3:
            send_to_telegram = st.checkbox("í…”ë ˆê·¸ë¨ ë°œì†¡", value=False)

        st.markdown("---")

        # ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True):
            if not selected_sources:
                st.error("ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return

            if not keywords and not fetch_all:
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ 'ì „ì²´ ìˆ˜ì§‘'ì„ ì²´í¬í•´ì£¼ì„¸ìš”.")
                return

            all_news = []
            summaries = {}

            progress_bar = st.progress(0)
            status_text = st.empty()

            total_steps = len(selected_sources)
            current_step = 0

            for source_name in selected_sources:
                current_step += 1
                status_text.text(f"ìˆ˜ì§‘ ì¤‘: {source_name}")
                progress_bar.progress(current_step / total_steps)

                if source_name == "ë„¤ì´ë²„ ë‰´ìŠ¤":
                    # ë„¤ì´ë²„ëŠ” í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                    for kw in keywords if keywords else ['ë‰´ìŠ¤']:
                        news_items = search_naver_news(kw, max_news)
                        if news_items:
                            all_news.extend(news_items)
                            st.success(f"âœ… ë„¤ì´ë²„ '{kw}': {len(news_items)}ê±´")

                            if use_gpt and news_items:
                                summary = summarize_korean_news(news_items, f"ë„¤ì´ë²„-{kw}")
                                if summary:
                                    summaries[f"ë„¤ì´ë²„-{kw}"] = summary
                        else:
                            st.info(f"â„¹ï¸ ë„¤ì´ë²„ '{kw}': ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        time.sleep(0.3)
                else:
                    # ì›¹ ìŠ¤í¬ë˜í•‘
                    scraper = SCRAPER_MAP.get(source_name)
                    if scraper:
                        news_items = scraper(keywords, max_news)
                        if news_items:
                            all_news.extend(news_items)
                            st.success(f"âœ… {source_name}: {len(news_items)}ê±´")

                            # GPT ìš”ì•½/ë²ˆì—­
                            source_info = NEWS_SOURCES.get(source_name, {})
                            if use_gpt:
                                if source_info.get('language') == 'en':
                                    summary = translate_and_summarize(news_items, source_name)
                                else:
                                    summary = summarize_korean_news(news_items, source_name)
                                if summary:
                                    summaries[source_name] = summary
                        else:
                            kw_text = f"'{', '.join(keywords)}'" if keywords else "ì „ì²´"
                            st.info(f"â„¹ï¸ {source_name}: {kw_text} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

                time.sleep(0.5)

            progress_bar.progress(1.0)
            status_text.text("ì™„ë£Œ!")

            # ê²°ê³¼ ì €ì¥
            st.session_state['news_results'] = all_news
            st.session_state['news_summaries'] = summaries

            if all_news:
                st.success(f"âœ… ì´ {len(all_news)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                st.error(f"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•˜ê±°ë‚˜, 'ì „ì²´ ìˆ˜ì§‘'ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")

            # í…”ë ˆê·¸ë¨ ë°œì†¡
            if send_to_telegram and all_news:
                msg = f"ğŸ“° <b>ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼</b>\n"
                msg += f"ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
                if keywords:
                    msg += f"í‚¤ì›Œë“œ: {', '.join(keywords)}\n\n"

                for source_key, summary in list(summaries.items())[:3]:
                    msg += f"<b>[{source_key}]</b>\n"
                    msg += f"{summary[:300]}...\n\n"

                if send_telegram(msg):
                    st.success("âœ… í…”ë ˆê·¸ë¨ ë°œì†¡ ì™„ë£Œ")
                else:
                    st.warning("âš ï¸ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨")

        # ê²°ê³¼ í‘œì‹œ
        if 'news_results' in st.session_state and st.session_state['news_results']:
            all_news = st.session_state['news_results']
            summaries = st.session_state.get('news_summaries', {})

            st.markdown("---")
            st.subheader(f"ğŸ“‹ ìˆ˜ì§‘ ê²°ê³¼ ({len(all_news)}ê±´)")

            # GPT ìš”ì•½/ë²ˆì—­
            if summaries:
                st.markdown("### ğŸ“ GPT ìš”ì•½/ë²ˆì—­")
                for source_key, summary in summaries.items():
                    with st.expander(f"**{source_key}**", expanded=False):
                        st.markdown(summary)

            st.markdown("### ğŸ“° ë‰´ìŠ¤ ëª©ë¡")

            # ì†ŒìŠ¤ í•„í„°
            sources_found = list(set(n.get('source', 'ê¸°íƒ€') for n in all_news))
            selected_filter = st.selectbox("ì†ŒìŠ¤ í•„í„°", ["ì „ì²´"] + sources_found)

            if selected_filter == "ì „ì²´":
                filtered_news = all_news
            else:
                filtered_news = [n for n in all_news if n.get('source') == selected_filter]

            # í…Œì´ë¸”
            if filtered_news:
                df_data = []
                for n in filtered_news:
                    # í•´ë‹¹ ì†ŒìŠ¤ì˜ ìš”ì•½ ì°¾ê¸°
                    source_name = n.get('source', '')
                    keyword = n.get('keyword', '')
                    summary_key = f"ë„¤ì´ë²„-{keyword}" if source_name == 'ë„¤ì´ë²„ ë‰´ìŠ¤' and keyword else source_name
                    summary_text = summaries.get(summary_key, '')

                    df_data.append({
                        'ì†ŒìŠ¤': source_name,
                        'ê¸°ì‚¬ì œëª©': n.get('title', ''),
                        'ì–¸ì–´': 'í•œêµ­ì–´' if n.get('language') == 'ko' else 'ì˜ì–´',
                        'ê¸°ì‚¬ì›ë¬¸URL': n.get('link', ''),
                        'ê¸°ì‚¬ìš”ì•½': summary_text[:200] if summary_text else ''
                    })

                df = pd.DataFrame(df_data)

                # í™”ë©´ í‘œì‹œìš© (ì—‘ì…€ê³¼ ë™ì¼í•œ ì»¬ëŸ¼)
                df_display = df.copy()
                df_display['ì–¸ì–´'] = df_display['ì–¸ì–´'].apply(lambda x: 'ğŸ‡°ğŸ‡·' if x == 'í•œêµ­ì–´' else 'ğŸ‡ºğŸ‡¸')

                # URL í´ë¦­ ê°€ëŠ¥í•˜ê²Œ í‘œì‹œ
                st.dataframe(
                    df_display,
                    use_container_width=True,
                    column_config={
                        "ì†ŒìŠ¤": st.column_config.TextColumn("ì†ŒìŠ¤", width="small"),
                        "ê¸°ì‚¬ì œëª©": st.column_config.TextColumn("ê¸°ì‚¬ì œëª©", width="large"),
                        "ì–¸ì–´": st.column_config.TextColumn("ì–¸ì–´", width="small"),
                        "ê¸°ì‚¬ì›ë¬¸URL": st.column_config.LinkColumn("ê¸°ì‚¬ì›ë¬¸URL", width="medium"),
                        "ê¸°ì‚¬ìš”ì•½": st.column_config.TextColumn("ê¸°ì‚¬ìš”ì•½", width="large"),
                    },
                    hide_index=True
                )

                # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ì „ì²´ ì»¬ëŸ¼)
                output = BytesIO()
                df.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)

                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ì†ŒìŠ¤/ì œëª©/ì–¸ì–´/URL/ìš”ì•½)",
                    data=output,
                    file_name=f"news_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            else:
                st.warning("í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        st.subheader("âš™ï¸ ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡")

        for group_name, sources in SOURCE_GROUPS.items():
            with st.expander(f"**{group_name}** ({len(sources)}ê°œ)", expanded=False):
                if group_name == "ë„¤ì´ë²„ ë‰´ìŠ¤":
                    st.markdown("**ğŸ‡°ğŸ‡· ë„¤ì´ë²„ ë‰´ìŠ¤**")
                    st.caption("ë„¤ì´ë²„ ë‰´ìŠ¤ API (í‚¤ì›Œë“œ ê²€ìƒ‰)")
                    st.caption("âœ… í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰ ì§€ì›")
                else:
                    for source_name in sources:
                        info = NEWS_SOURCES.get(source_name, {})
                        lang = "ğŸ‡°ğŸ‡·" if info.get('language') == 'ko' else "ğŸ‡ºğŸ‡¸"
                        search_type = info.get('search_type', 'main')

                        st.markdown(f"**{lang} {source_name}**")
                        st.caption(f"{info.get('description', '')}")
                        st.caption(f"ğŸ”— {info.get('url', '')}")

                        if search_type == 'url':
                            st.caption("âœ… í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰ ì§€ì›")
                        else:
                            st.caption("ğŸ“„ ë©”ì¸ í˜ì´ì§€ ìˆ˜ì§‘ (í‚¤ì›Œë“œ í•„í„°ë§)")

                        st.markdown("---")

        st.info("ğŸ’¡ ì†ŒìŠ¤ ì¶”ê°€ ìš”ì²­ì€ í”¼ë“œë°± í˜ì´ì§€ì—ì„œ í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
